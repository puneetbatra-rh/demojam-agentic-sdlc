{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27879ff3-9691-4d85-bfb3-ae539693b4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 05:53:42.456 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.458 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.482 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.483 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.484 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.485 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.487 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.488 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.488 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-02 05:53:42.489 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m st.markdown(\u001b[33m\"\"\"\u001b[39m\u001b[33m## ğŸ”„ Progress Tracker\u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     16\u001b[39m st.header(\u001b[33m\"\u001b[39m\u001b[33mğŸ“‹ User Requirements\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m requirements = st.text_area(\u001b[33m\"\u001b[39m\u001b[33mEnter Requirements:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mstate\u001b[49m.get(\u001b[33m\"\u001b[39m\u001b[33mrequirements\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m), key=\u001b[33m\"\u001b[39m\u001b[33mrequirements_input\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'state' is not defined"
     ]
    }
   ],
   "source": [
    "# app.py\n",
    "import os\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "st.set_page_config(page_title=\"AI SDLC Wizard\", layout=\"wide\")\n",
    "logo_path = \"images/logo.png\" \n",
    "st.image(logo_path, width=200)\n",
    "st.title(\"DemoJam Red Hat One 2026\")\n",
    "\n",
    "st.markdown(\"\"\"## Agentic AI System for Software Deveopment Lifycle Automation\"\"\")\n",
    "\n",
    "\n",
    "st.markdown(\"\"\"## ğŸ”„ Progress Tracker\"\"\")\n",
    "\n",
    "st.header(\"ğŸ“‹ User Requirements\")\n",
    "requirements = st.text_area(\"Enter Requirements:\", state.get(\"requirements\", \"\"), key=\"requirements_input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2bf7e5-49ac-494a-92b2-87001100d752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 15:57:14.518 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.521 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.522 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.522 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.523 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.525 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-01 15:57:14.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "from_tools",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m st.markdown(\u001b[33m\"\u001b[39m\u001b[33m---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Initialize the agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m agent = \u001b[43minitialize_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Initialize chat history\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m st.session_state:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demojam-agentic-sdlc/.venv/lib64/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:219\u001b[39m, in \u001b[36mCachedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    217\u001b[39m         spinner_message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(...)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_or_create_cached_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspinner_message\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demojam-agentic-sdlc/.venv/lib64/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:261\u001b[39m, in \u001b[36mCachedFunc._get_or_create_cached_value\u001b[39m\u001b[34m(self, func_args, func_kwargs, spinner_message)\u001b[39m\n\u001b[32m    255\u001b[39m spinner_or_no_context = (\n\u001b[32m    256\u001b[39m     spinner(spinner_message, _cache=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spinner_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_nested_cache_function\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext()\n\u001b[32m    259\u001b[39m )\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m spinner_or_no_context:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_cache_miss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demojam-agentic-sdlc/.venv/lib64/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py:320\u001b[39m, in \u001b[36mCachedFunc._handle_cache_miss\u001b[39m\u001b[34m(self, cache, value_key, func_args, func_kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# We acquired the lock before any other thread. Compute the value!\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._info.cached_message_replay_ctx.calling_cached_function(\n\u001b[32m    318\u001b[39m     \u001b[38;5;28mself\u001b[39m._info.func\n\u001b[32m    319\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     computed_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# We've computed our value, and now we need to write it back to the cache\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# along with any \"replay messages\" that were generated during value computation.\u001b[39;00m\n\u001b[32m    324\u001b[39m messages = \u001b[38;5;28mself\u001b[39m._info.cached_message_replay_ctx._most_recent_messages\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36minitialize_agent\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     45\u001b[39m llm = OpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0.2\u001b[39m) \u001b[38;5;66;03m# Using a capable model for function calling\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# The Agent Worker uses the LLM to decide on actions (tool calls or direct answer)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m worker = \u001b[43mFunctionAgent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_tools\u001b[49m(\n\u001b[32m     49\u001b[39m     tools=tools,\n\u001b[32m     50\u001b[39m     llm=llm,\n\u001b[32m     51\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Show the LLM's thought process\u001b[39;00m\n\u001b[32m     52\u001b[39m     system_prompt=(\n\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are a sophisticated AI Orchestrator that takes user requirements \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand decides on the best next step. You must strictly use the provided tools \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mif the request involves market analysis or project specification drafting. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf the request is a general question, answer directly. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mALWAYS consider the user\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms input as their \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrequirements\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m     )\n\u001b[32m     59\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# The AgentRunner provides the interface to run the worker\u001b[39;00m\n\u001b[32m     62\u001b[39m agent = AgentRunner(worker)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demojam-agentic-sdlc/.venv/lib64/python3.11/site-packages/pydantic/_internal/_model_construction.py:289\u001b[39m, in \u001b[36mModelMetaclass.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m private_attributes \u001b[38;5;129;01mand\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m private_attributes:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m private_attributes[item]\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(item)\n",
      "\u001b[31mAttributeError\u001b[39m: from_tools"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define MCP (Tool) Functions ---\n",
    "# These simulate external tools or services (your MCP tools)\n",
    "\n",
    "def research_market_trends(product_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes current market trends, competitor activity, and search volume \n",
    "    for a specific product or industry.\n",
    "    \"\"\"\n",
    "    if \"AI\" in product_name.upper():\n",
    "        return \"Market analysis for AI: High demand, rapid innovation, key competitors are Google, Meta, and Anthropic. Focus on niche applications for differentiation.\"\n",
    "    elif \"E-commerce\" in product_name.upper():\n",
    "        return \"Market analysis for E-commerce: Steady growth, key trend is mobile-first shopping and personalized recommendations. Strong SEO and user experience are critical.\"\n",
    "    else:\n",
    "        return f\"Market research for {product_name}: Requires further web search, but general sentiment is positive. Consider a full-scale analysis.\"\n",
    "\n",
    "def generate_initial_project_spec(requirements: str) -> str:\n",
    "    \"\"\"\n",
    "    Drafts an initial project specification, including key features and \n",
    "    technical stack recommendations based on a high-level requirements summary.\n",
    "    \"\"\"\n",
    "    if \"chatbot\" in requirements.lower():\n",
    "        return \"Initial Spec: Core feature is a conversational UI (Streamlit/React). Technical stack: Python, LlamaIndex/LangChain for Agent orchestration, Llama-3 LLM, Vector DB for RAG. Key features: user authentication, multi-turn chat, document Q&A.\"\n",
    "    elif \"data dashboard\" in requirements.lower():\n",
    "        return \"Initial Spec: Core feature is interactive data visualization (Streamlit/Dash). Technical stack: Python, Pandas, Matplotlib/Plotly, PostgreSQL for data storage. Key features: filterable data views, user login, scheduled data updates.\"\n",
    "    else:\n",
    "        return f\"Initial Spec for '{requirements}': Insufficient detail. Suggest holding a discovery workshop to define scope and key feature list.\"\n",
    "\n",
    "# --- 2. Create LlamaIndex Tools ---\n",
    "\n",
    "market_research_tool = FunctionTool.from_defaults(fn=research_market_trends)\n",
    "spec_generation_tool = FunctionTool.from_defaults(fn=generate_initial_project_spec)\n",
    "\n",
    "# List of tools the Orchestrator LLM can choose from\n",
    "tools = [market_research_tool, spec_generation_tool]\n",
    "\n",
    "# --- 3. Initialize the Orchestrator Agent (Llama Stack Core Logic) ---\n",
    "\n",
    "@st.cache_resource\n",
    "def initialize_agent():\n",
    "    \"\"\"Initializes and caches the LlamaIndex Agent.\"\"\"\n",
    "    # The Orchestrator LLM\n",
    "    llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2) # Using a capable model for function calling\n",
    "\n",
    "    # The Agent Worker uses the LLM to decide on actions (tool calls or direct answer)\n",
    "    worker = FunctionAgent.from_tools(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        verbose=True, # Show the LLM's thought process\n",
    "        system_prompt=(\n",
    "            \"You are a sophisticated AI Orchestrator that takes user requirements \"\n",
    "            \"and decides on the best next step. You must strictly use the provided tools \"\n",
    "            \"if the request involves market analysis or project specification drafting. \"\n",
    "            \"If the request is a general question, answer directly. \"\n",
    "            \"ALWAYS consider the user's input as their 'requirements'.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # The AgentRunner provides the interface to run the worker\n",
    "    agent = AgentRunner(worker)\n",
    "    return agent\n",
    "\n",
    "# --- 4. Streamlit Application UI ---\n",
    "\n",
    "st.set_page_config(page_title=\"Llama Stack Agentic Requirements Orchestrator\", layout=\"wide\")\n",
    "\n",
    "st.title(\"ğŸ§  Agentic AI Requirements Orchestrator (Llama Stack) ğŸ“\")\n",
    "st.markdown(\"Enter your project requirements. The AI **Orchestrator LLM** will decide which **tools** or **LLMs** to use to fulfill your request.\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Initialize the agent\n",
    "agent = initialize_agent()\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Main chat input\n",
    "if prompt := st.chat_input(\"Enter your project requirements (e.g., 'I need a new AI chatbot with market data integration' or 'What is 5 + 5?'):\"):\n",
    "    # Display user message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Get agent response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Agent is orchestrating the response...\"):\n",
    "            # Run the agent with the user's input\n",
    "            response = agent.run(prompt)\n",
    "            \n",
    "            # Display the final response\n",
    "            final_response = response.response\n",
    "            st.markdown(final_response)\n",
    "            \n",
    "            # Optionally display the Agent's thought process for debugging/transparency\n",
    "            st.divider()\n",
    "            st.subheader(\"Orchestrator Thought Process (Verbose)\")\n",
    "            st.code(response.source_nodes[0].text if response.source_nodes else \"N/A - Direct Answer\", language=\"markdown\")\n",
    "\n",
    "\n",
    "    # Add assistant response to chat history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": final_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404aa555-5762-436f-bfec-4f57681be333",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2898320323.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install streamlit\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135ccbd2-c83e-4c55-86be-18158ff8b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.14.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index)\n",
      "  Downloading llama_index_cli-0.5.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting llama-index-core<0.15.0,>=0.14.7 (from llama-index)\n",
      "  Downloading llama_index_core-0.14.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-index-llms-openai<0.7,>=0.6.0 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.6.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.5.4-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting aiohttp<4,>=3.8.6 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting aiosqlite (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.2.0 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: httpx in ./.venv/lib64/python3.11/site-packages (from llama-index-core<0.15.0,>=0.14.7->llama-index) (0.28.1)\n",
      "Collecting llama-index-workflows!=2.9.0,<3,>=2 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading llama_index_workflows-2.10.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in ./.venv/lib64/python3.11/site-packages (from llama-index-core<0.15.0,>=0.14.7->llama-index) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib64/python3.11/site-packages (from llama-index-core<0.15.0,>=0.14.7->llama-index) (2.2.5)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib64/python3.11/site-packages (from llama-index-core<0.15.0,>=0.14.7->llama-index) (11.2.1)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib64/python3.11/site-packages (from llama-index-core<0.15.0,>=0.14.7->llama-index) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./.venv/lib64/python3.11/site-packages (from llama-index-core<0.15.0,>=0.14.7->llama-index) (2.10.6)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in ./.venv/lib64/python3.11/site-packages (from llama-index-core<0.15.0,>=0.14.7->llama-index) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib64/python3.11/site-packages (from llama-index-core<0.15.0,>=0.14.7->llama-index) (2.32.3)\n",
      "Collecting setuptools>=80.9.0 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in ./.venv/lib64/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.7->llama-index) (2.0.40)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./.venv/lib64/python3.11/site-packages (from llama-index-core<0.15.0,>=0.14.7->llama-index) (9.1.2)\n",
      "Collecting tiktoken>=0.7.0 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm<5,>=4.66.1 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib64/python3.11/site-packages (from llama-index-core<0.15.0,>=0.14.7->llama-index) (4.13.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading wrapt-2.0.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib64/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama-index) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib64/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.7->llama-index) (3.1.6)\n",
      "Collecting openai>=1.1.0 (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index)\n",
      "  Downloading openai-2.6.1-py3-none-any.whl.metadata (29 kB)\n",
      "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting beautifulsoup4<5,>=4.12.3 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting defusedxml>=0.7.1 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: pandas<2.3.0 in ./.venv/lib64/python3.11/site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.3)\n",
      "Collecting pypdf<7,>=5.1.0 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
      "  Downloading pypdf-6.1.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pydantic>=2.8.0 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib64/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib64/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index)\n",
      "  Downloading jiter-0.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib64/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib64/python3.11/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib64/python3.11/site-packages (from httpx->llama-index-core<0.15.0,>=0.14.7->llama-index) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib64/python3.11/site-packages (from httpx->llama-index-core<0.15.0,>=0.14.7->llama-index) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib64/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.7->llama-index) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib64/python3.11/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib64/python3.11/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib64/python3.11/site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib64/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.7->llama-index) (0.7.0)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading pydantic_core-2.41.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting wrapt (from llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading wrapt-1.17.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.77-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.77 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.77-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: click<9,>=8.1.7 in ./.venv/lib64/python3.11/site-packages (from llama-cloud-services>=0.6.77->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.2.0)\n",
      "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.76-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.76 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.76-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.75-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.75 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.75-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.74-py3-none-any.whl.metadata (6.6 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-cloud-services>=0.6.74 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.74-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.73-py3-none-any.whl.metadata (6.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting llama-cloud-services>=0.6.73 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.73-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.72-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.72 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.72-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.71-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.71 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.71-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.70-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.70 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.70-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.69-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.69 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.69-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.68-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.68 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.68-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.67-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.67 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.67-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.66-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.66 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.66-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.65-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.64 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.65-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading llama_cloud_services-0.6.64-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.64-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading llama_parse-0.6.63-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.63 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.63-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in ./.venv/lib64/python3.11/site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index)\n",
      "  Downloading regex-2025.10.23-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib64/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.7->llama-index) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib64/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.7->llama-index) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib64/python3.11/site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.7->llama-index) (3.2.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib64/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15.0,>=0.14.7->llama-index) (24.2)\n",
      "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.7->llama-index)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib64/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.7->llama-index) (3.0.2)\n",
      "Downloading llama_index-0.14.7-py3-none-any.whl (7.4 kB)\n",
      "Downloading llama_index_cli-0.5.3-py3-none-any.whl (28 kB)\n",
      "Downloading llama_index_core-0.14.7-py3-none-any.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m189.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m337.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
      "Downloading llama_index_llms_openai-0.6.6-py3-none-any.whl (26 kB)\n",
      "Downloading llama_index_readers_file-0.5.4-py3-none-any.whl (51 kB)\n",
      "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Downloading llama_index_workflows-2.10.0-py3-none-any.whl (90 kB)\n",
      "Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (359 kB)\n",
      "Downloading pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
      "Downloading pydantic_core-2.41.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m287.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-6.1.3-py3-none-any.whl (323 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl (17 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
      "Downloading wrapt-1.17.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (82 kB)\n",
      "Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl (3.2 kB)\n",
      "Downloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
      "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m269.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "Downloading regex-2025.10.23-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.3/800.3 kB\u001b[0m \u001b[31m287.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m251.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Downloading tiktoken-0.12.0-cp311-cp311-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m272.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Installing collected packages: striprtf, filetype, dirtyjson, wrapt, typing-extensions, tqdm, soupsieve, setuptools, regex, pypdf, propcache, networkx, mypy-extensions, multidict, marshmallow, joblib, jiter, fsspec, frozenlist, defusedxml, colorama, aiohappyeyeballs, yarl, typing-inspection, typing-inspect, tiktoken, pydantic-core, nltk, griffe, deprecated, beautifulsoup4, aiosqlite, aiosignal, pydantic, dataclasses-json, aiohttp, openai, llama-index-instrumentation, llama-cloud, banks, llama-index-workflows, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-cli, llama-index-readers-llama-parse, llama-index\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.13.2\n",
      "\u001b[2K    Uninstalling typing_extensions-4.13.2:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.13.2â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/51\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: setuptoolsâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/51\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: setuptools 65.5.1â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/51\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling setuptools-65.5.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/51\u001b[0m [setuptools]ns]\n",
      "\u001b[2K      Successfully uninstalled setuptools-65.5.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/51\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: pydantic-core[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/51\u001b[0m [typing-inspection]\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.27.2â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/51\u001b[0m [typing-inspection]\n",
      "\u001b[2K    Uninstalling pydantic_core-2.27.2:90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/51\u001b[0m [typing-inspection]\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.27.2â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/51\u001b[0m [typing-inspection]\n",
      "\u001b[2K  Attempting uninstall: pydantic[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/51\u001b[0m [beautifulsoup4]\n",
      "\u001b[2K    Found existing installation: pydantic 2.10.6â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/51\u001b[0m [beautifulsoup4]\n",
      "\u001b[2K    Uninstalling pydantic-2.10.6:91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/51\u001b[0m [beautifulsoup4]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.10.6â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30/51\u001b[0m [beautifulsoup4]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51/51\u001b[0m [llama-index]ma-index]dices-managed-llama-cloud]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 aiosqlite-0.21.0 banks-2.2.0 beautifulsoup4-4.14.2 colorama-0.4.6 dataclasses-json-0.6.7 defusedxml-0.7.1 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 frozenlist-1.8.0 fsspec-2025.10.0 griffe-1.14.0 jiter-0.11.1 joblib-1.5.2 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.14.7 llama-index-cli-0.5.3 llama-index-core-0.14.7 llama-index-embeddings-openai-0.5.1 llama-index-indices-managed-llama-cloud-0.9.4 llama-index-instrumentation-0.4.2 llama-index-llms-openai-0.6.6 llama-index-readers-file-0.5.4 llama-index-readers-llama-parse-0.5.1 llama-index-workflows-2.10.0 llama-parse-0.6.54 marshmallow-3.26.1 multidict-6.7.0 mypy-extensions-1.1.0 networkx-3.5 nltk-3.9.2 openai-1.109.1 propcache-0.4.1 pydantic-2.12.3 pydantic-core-2.41.4 pypdf-6.1.3 regex-2025.10.23 setuptools-80.9.0 soupsieve-2.8 striprtf-0.0.26 tiktoken-0.12.0 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2 wrapt-1.17.3 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9de6c1-1c5a-479d-b7e2-968bb70cdab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangGraph",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
