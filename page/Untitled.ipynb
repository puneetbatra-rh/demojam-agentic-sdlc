{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d2193-41af-4fef-9efd-4994fb7ff1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client import RAGDocument\n",
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "from termcolor import cprint\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "import uuid\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import streamlit as st\n",
    "\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sidebar configurations\n",
    "with st.sidebar:\n",
    "    st.header(\"Configuration\")\n",
    "    available_models = client.models.list()\n",
    "    available_models = [model.identifier for model in available_models if model.model_type == \"llm\"]\n",
    "    selected_model = st.selectbox(\n",
    "        \"Choose a model\",\n",
    "        available_models,\n",
    "        index=0,\n",
    "    )\n",
    "\n",
    "    temperature = st.slider(\n",
    "        \"Temperature\",\n",
    "        min_value=0.0,\n",
    "        max_value=1.0,\n",
    "        value=0.0,\n",
    "        step=0.1,\n",
    "        help=\"Controls the randomness of the response. Higher values make the output more creative and unexpected, lower values make it more conservative and predictable\",\n",
    "    )\n",
    "\n",
    "    top_p = st.slider(\n",
    "        \"Top P\",\n",
    "        min_value=0.0,\n",
    "        max_value=1.0,\n",
    "        value=0.95,\n",
    "        step=0.1,\n",
    "    )\n",
    "\n",
    "    max_tokens = st.slider(\n",
    "        \"Max Tokens\",\n",
    "        min_value=0,\n",
    "        max_value=4096,\n",
    "        value=512,\n",
    "        step=1,\n",
    "        help=\"The maximum number of tokens to generate\",\n",
    "    )\n",
    "\n",
    "    repetition_penalty = st.slider(\n",
    "        \"Repetition Penalty\",\n",
    "        min_value=1.0,\n",
    "        max_value=2.0,\n",
    "        value=1.0,\n",
    "        step=0.1,\n",
    "        help=\"Controls the likelihood for generating the same word or phrase multiple times in the same sentence or paragraph. 1 implies no penalty, 2 will strongly discourage model to repeat words or phrases.\",\n",
    "    )\n",
    "\n",
    "    stream = st.checkbox(\"Stream\", value=True)\n",
    "    system_prompt = st.text_area(\n",
    "        \"System Prompt\",\n",
    "        value=\"You are a helpful AI assistant.\",\n",
    "        help=\"Initial instructions given to the AI to set its behavior and context\",\n",
    "    )\n",
    "\n",
    "    # Add clear chat button to sidebar\n",
    "    if st.button(\"Clear Chat\", use_container_width=True):\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "\n",
    "\n",
    "# Main chat interface\n",
    "st.title(\"ðŸ¦™ Chat\")\n",
    "\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"Example: What is Llama Stack?\"):\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Display user message\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Display assistant response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        full_response = \"\"\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            strategy = {\n",
    "                \"type\": \"top_p\",\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "            }\n",
    "        else:\n",
    "            strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "        response = client.inference.chat_completion(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            model_id=selected_model,\n",
    "            stream=stream,\n",
    "            sampling_params={\n",
    "                \"strategy\": strategy,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"repetition_penalty\": repetition_penalty,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        if stream:\n",
    "            for chunk in response:\n",
    "                if chunk.event.event_type == \"progress\":\n",
    "                    full_response += chunk.event.delta.text\n",
    "                message_placeholder.markdown(full_response + \"â–Œ\")\n",
    "            message_placeholder.markdown(full_response)\n",
    "        else:\n",
    "            full_response = response.completion_message.content\n",
    "            message_placeholder.markdown(full_response)\n",
    "\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
